{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12964c20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bhuwanthapa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec149af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhuwanthapa/opt/anaconda3/lib/python3.9/site-packages/ebooklib/epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "#using epubs file\n",
    "def epub_to_text(file_path):\n",
    "    book = epub.read_epub(file_path)\n",
    "    text = \"\"\n",
    "\n",
    "    for item in book.get_items():\n",
    "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "            soup = BeautifulSoup(item.get_content(), 'html.parser')\n",
    "            text += soup.get_text() + \"\\n\"\n",
    "            \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Tokenize words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove words that are less than 3 characters long\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "\n",
    "    # Join the filtered words back into a string\n",
    "    filtered_text = ' '.join(words)\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "epub_files = [\n",
    "      'Think-And-Grow-Rich.epub',\n",
    "      'CompTIA-Cybersecurity-Analyst.epub',\n",
    "      'Education-AND-the-good-ife-by-Bertrand-Russell.epub',\n",
    "      'Rich-Dad-Poor-Dad.epub',\n",
    "      'The-five-Second-Rule-Transform-your-Life-Work-AND-Confidence-WITH-Everyday-Courage.epub',\n",
    "      'The-Kite-Runner.epub',\n",
    "      'Safety-Health-AND-Environmental-Handbook.epub',\n",
    "      'Strategic-Management-AND-Business-Policy.epub',\n",
    "     'The-Complete-Art-of-War.epub',\n",
    "       'Encyclopedia-of-Physical-Science-(Facts-on-File-Science-Library)-Volume-1-AND-2.epub',\n",
    "       'The-Oxford-Handbook-of-Contextual-Political-Analysis-(Oxford-Handbooks-of-Political-Science).epub',\n",
    "       'Trends-in-Computer-Science-Engineering-and-Information-Technology-First-International-Conference-on-Computer-Science-Engineering and-Information-Technology-CCSEIT-2011-Tirunelveli-Tamil-Nadu-India-September-23-25-2011-Proceedings.epub',\n",
    "       'Oxford-English-Dictionary.epub',\n",
    "    #    'The-Animal-Book.epub'\n",
    "]\n",
    "corpus = ''\n",
    "\n",
    "for epub_file in epub_files:\n",
    "    text = epub_to_text(epub_file)\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    corpus += preprocessed_text + \"\\n\"\n",
    "\n",
    "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a473520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 176074), ('and', 84612), ('that', 28202), ('for', 28132), ('with', 21607), ('are', 16552), ('from', 15923), ('this', 13939), ('not', 12549), ('which', 10849)]\n"
     ]
    }
   ],
   "source": [
    "#proprocessing\n",
    "from collections import Counter\n",
    "\n",
    "def word_frequency(corpus):\n",
    "    words = corpus.split()\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts.most_common()\n",
    "\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "word_counts = word_frequency(corpus)\n",
    "\n",
    "# Save the word frequencies to a CSV file\n",
    "import csv\n",
    "\n",
    "with open('word_frequencies.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['word', 'count'])\n",
    "    \n",
    "    for word, count in word_counts:\n",
    "        csv_writer.writerow([word, count])\n",
    "\n",
    "print(word_counts[:10])  # Print the 10 most common words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ab36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting the model training code now and importing the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0eefd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5c52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe\n",
    "data = pd.read_csv(\"word_frequencies.csv\")\n",
    "# Remove rows with NaN values\n",
    "data.dropna(subset=['word'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b414aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data by creating a bag of words\n",
    "words = list(set(''.join(data['word'])))\n",
    "word_to_index = {word: i for i, word in enumerate(words)}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11ed8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the word strings to vectors\n",
    "def word_to_vector(word):\n",
    "    vec = np.zeros(len(words), dtype=np.int32)\n",
    "    for char in word:\n",
    "        vec[word_to_index[char]] += 1\n",
    "    return vec\n",
    "\n",
    "X = np.array([word_to_vector(word) for word in data['word']])\n",
    "y = np.array(data['word'])\n",
    "counts = np.array(data['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b60b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████████| 106554/106554 [03:12<00:00, 554.83it/s]\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "        self.class_probs = None\n",
    "        self.cond_probs = None\n",
    "    \n",
    "    def fit(self, X, y, counts):\n",
    "        n, d = X.shape\n",
    "        self.class_probs = np.zeros(len(self.classes))\n",
    "        self.cond_probs = np.zeros((len(self.classes), d))\n",
    "\n",
    "        for i, c in enumerate(tqdm(self.classes, desc=\"Processing classes\")):\n",
    "            X_c = X[y == c]\n",
    "            n_c = counts[y == c].sum()\n",
    "            self.class_probs[i] = n_c / counts.sum()\n",
    "            self.cond_probs[i] = (X_c.sum(axis=0) + 1) / (X_c.sum() + d)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = np.zeros((len(X), len(self.classes)))\n",
    "        \n",
    "        for i, x in enumerate(X):\n",
    "            for j, c in enumerate(self.classes):\n",
    "                probs[i, j] = np.log(self.class_probs[j]) + (np.log(self.cond_probs[j]) * x).sum()\n",
    "        \n",
    "        return self.classes[np.argmax(probs, axis=1)]\n",
    "\n",
    "    def predict_starting_with(self, X, prefix, n=10):\n",
    "        probs = np.zeros((len(X), len(self.classes)))\n",
    "        for i, x in enumerate(X):\n",
    "            for j, c in enumerate(self.classes):\n",
    "                if c.startswith(prefix):\n",
    "                    probs[i, j] = np.log(self.class_probs[j]) + (np.log(self.cond_probs[j]) * x).sum()\n",
    "                else:\n",
    "                    probs[i, j] = -np.inf\n",
    "        return probs\n",
    "    def predict_top_n_words(self, X, prefix, n=4):\n",
    "        probs = self.predict_starting_with(X, prefix)\n",
    "        top_n_indices = np.argsort(-probs, axis=1)[:, :n]\n",
    "        top_n_words = self.classes[top_n_indices][0]\n",
    "        top_n_probs = np.exp(probs[0][top_n_indices[0]])\n",
    "        top_n_probs = top_n_probs / top_n_probs.sum()\n",
    "        return list(zip(top_n_words, top_n_probs))\n",
    "\n",
    "clf = NaiveBayes(classes=np.unique(y))\n",
    "clf.fit(X, y, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c1d2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(input_str, n=4):\n",
    "    input_str = input_str.lower()\n",
    "    input_vec = np.array([word_to_vector(input_str)])\n",
    "    preds = clf.predict_top_n_words(input_vec, input_str, n)\n",
    "    return preds if preds else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "add26449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('catch', 0.300355866398384), ('category', 0.24663037054406103), ('cat', 0.2301062405668244), ('categories', 0.22290752249073056)]\n"
     ]
    }
   ],
   "source": [
    "print(predict_word(\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4516be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c54a3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd63f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a9c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7793c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "import pickle\n",
    "with open(\"naive_bayes_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d18792d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Assuming you have the 'data' DataFrame with the 'word' column\n",
    "words = list(set(''.join(data['word'])))\n",
    "word_to_index = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "# Save the word_to_index dictionary as a pickle file\n",
    "with open(\"word_to_index.pkl\", \"wb\") as word_to_index_file:\n",
    "    pickle.dump(word_to_index, word_to_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d7d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
